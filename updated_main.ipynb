{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8oadoI9Piwry",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31027,
     "status": "ok",
     "timestamp": 1734438608100,
     "user": {
      "displayName": "Hyounggyu Kim",
      "userId": "06810140716652608120"
     },
     "user_tz": -540
    },
    "id": "8oadoI9Piwry",
    "outputId": "620cc8ee-16c6-4a75-8d59-ade14386c9a4"
   },
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94dbe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy scipy tensorflow pandas scikit-learn bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04475f-5b04-4f44-91e4-00583b958f78",
   "metadata": {
    "id": "3e04475f-5b04-4f44-91e4-00583b958f78"
   },
   "source": [
    "## Import Libraries and Functions and Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kCcB6_Mqi9iQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9863,
     "status": "ok",
     "timestamp": 1734438619905,
     "user": {
      "displayName": "Hyounggyu Kim",
      "userId": "06810140716652608120"
     },
     "user_tz": -540
    },
    "id": "kCcB6_Mqi9iQ",
    "outputId": "376027ae-7b61-452d-e8c9-165f81daf143"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "#, Add, Subtract\n",
    "from bokeh.layouts import row\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh import palettes\n",
    "\n",
    "import random\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# functions\n",
    "#mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def set_seeds(seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def set_global_determinism(seed=0):\n",
    "    set_seeds(seed=seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# figure font adjustments\n",
    "def figure_settings(fig,label_font_size='12pt',legend_fix=True):\n",
    "    fig.yaxis.axis_label_text_font_size  = '16pt'\n",
    "    fig.xaxis.major_label_text_font_size = label_font_size\n",
    "    fig.yaxis.major_label_text_font_size = label_font_size\n",
    "    fig.yaxis.major_label_text_font_style = \"bold\"\n",
    "    fig.xaxis.major_label_text_font_style = \"bold\"\n",
    "    fig.xaxis.axis_label_text_font_size  = '16pt'\n",
    "    fig.axis.axis_label_text_font_style = 'bold'\n",
    "\n",
    "    # fig.legend.location = 'bottom_right'\n",
    "    if legend_fix:\n",
    "        fig.legend.label_text_font_size = '14pt'\n",
    "        fig.legend.title_text_font = 'Arial'\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a978bf6c-0334-4ae9-8226-21a7b61bef9d",
   "metadata": {
    "executionInfo": {
     "elapsed": 445,
     "status": "ok",
     "timestamp": 1734438624047,
     "user": {
      "displayName": "Hyounggyu Kim",
      "userId": "06810140716652608120"
     },
     "user_tz": -540
    },
    "id": "a978bf6c-0334-4ae9-8226-21a7b61bef9d"
   },
   "outputs": [],
   "source": [
    "# Physics Informed Neural Network with Taylor Series\n",
    "'''\n",
    "N_INPUT: The number of input bio-z dimensions for one heartbeat\n",
    "N_FEAT: The number of physiological features\n",
    "N_EXT: The number of features extracted by the CNN\n",
    "'''\n",
    "\n",
    "def model_DNN(N_INPUT, N_FEAT=1, N_EXT=100):\n",
    "    # The input to the model is a 1D tensor representing a time series of heartbeat data, sampled with 250/8 points for 30 seconds\n",
    "    inp_beat=tf.keras.Input(shape=(N_INPUT,))\n",
    "\n",
    "    # Define the 1D CNN for NN feature extraction\n",
    "    # The input tensor is first expanded by one dimension (from 1D to 2D) to be compatible with the Conv1D layer\n",
    "    cnn1_1 = tf.keras.layers.Conv1D(32,5,activation='relu')(tf.keras.layers.Reshape((N_INPUT, 1))(inp_beat))\n",
    "    cnn1_2 = tf.keras.layers.Conv1D(64,3,activation='relu')(cnn1_1)\n",
    "    mp_cnn1 = tf.keras.layers.MaxPooling1D(pool_size=3,strides=1)(cnn1_2)\n",
    "    fl_cnn1 = tf.keras.layers.Flatten()(mp_cnn1)\n",
    "\n",
    "    # A fully connected layer further processes the flattened tensor and extracts N_EXT features\n",
    "    feat_ext = tf.keras.layers.Dense(N_EXT,activation='relu')(fl_cnn1)\n",
    "\n",
    "    # Define physiological features (case study uses 3 features), each of these features is expected to be a 1D tensor\n",
    "    inp_feat1 = tf.keras.Input(shape=(N_FEAT,)) # feat 1\n",
    "    inp_feat2 = tf.keras.Input(shape=(N_FEAT,)) # feat 2\n",
    "    inp_feat3 = tf.keras.Input(shape=(N_FEAT,)) # feat 3\n",
    "\n",
    "    # The extracted features and physiological features are concatenated together\n",
    "    feat_comb = tf.keras.layers.Concatenate()([inp_feat1,inp_feat2,inp_feat3,feat_ext])\n",
    "\n",
    "    # A fully connected layer with is applied to the concatenated features\n",
    "    dense1_1 = tf.keras.layers.Dense(60,activation='relu')(feat_comb)\n",
    "    out = tf.keras.layers.Dense(N_FEAT)(dense1_1)\n",
    "\n",
    "    # Finally, the model is instantiated with the specified inputs and outputs\n",
    "    model = tf.keras.Model(inputs=[inp_beat, inp_feat1, inp_feat2, inp_feat3], outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3147e93-a796-4077-b31c-248be2d20914",
   "metadata": {
    "id": "c3147e93-a796-4077-b31c-248be2d20914"
   },
   "source": [
    "### Import a Demo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "199f70e4-06d2-4848-9ae3-57deed7785bf",
   "metadata": {
    "executionInfo": {
     "elapsed": 1462,
     "status": "ok",
     "timestamp": 1734438628273,
     "user": {
      "displayName": "Hyounggyu Kim",
      "userId": "06810140716652608120"
     },
     "user_tz": -540
    },
    "id": "199f70e4-06d2-4848-9ae3-57deed7785bf"
   },
   "outputs": [],
   "source": [
    "# load an example data for demo\n",
    "df_demo_data = pd.read_pickle('data_demo_pinn_bioz_bp',compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a95ce-5e05-4499-a4da-27394b896c3f",
   "metadata": {
    "id": "a40a95ce-5e05-4499-a4da-27394b896c3f"
   },
   "source": [
    "### Preprocess and Prepare the Train/Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb49b2b3-be56-4654-b4ec-0134c8486bf3",
   "metadata": {
    "executionInfo": {
     "elapsed": 964,
     "status": "ok",
     "timestamp": 1734438630799,
     "user": {
      "displayName": "Hyounggyu Kim",
      "userId": "06810140716652608120"
     },
     "user_tz": -540
    },
    "id": "bb49b2b3-be56-4654-b4ec-0134c8486bf3"
   },
   "outputs": [],
   "source": [
    "# Initialize a SEED value to ensure that the random processes in the code can be reproduced.\n",
    "SEED = 123\n",
    "\n",
    "# Call the function with seed value\n",
    "set_global_determinism(seed=SEED)\n",
    "\n",
    "# The keys for the beat data (beat_key), the target (out_key), and the features (feat_keys) are defined\n",
    "beat_key = 'bioz_beats'\n",
    "out_key = 'sys'\n",
    "feat_keys = ['phys_feat_1','phys_feat_2','phys_feat_3']\n",
    "\n",
    "# Data scaling of BP, input beats, and input features\n",
    "# This scaler standardizes by removing the mean and scaling to unit variance\n",
    "# This is done to ensure having the same scale, which can improve the performance of machine learning algorithms\n",
    "scaler_out = preprocessing.StandardScaler().fit(df_demo_data[out_key].to_numpy()[:, None])\n",
    "scaler_beats = preprocessing.StandardScaler().fit(np.concatenate(df_demo_data[beat_key].to_numpy())[:, None])\n",
    "scaler_X = [preprocessing.StandardScaler().fit(df_demo_data[a].to_numpy()[:, None]) for a in feat_keys]\n",
    "\n",
    "# Apply Scaling\n",
    "# The scaled versions of the BP, input beats, and input features are then added to the dataframe\n",
    "df_demo_data.loc[df_demo_data.index,beat_key+'_scaled'] = df_demo_data.apply(lambda x: np.concatenate(scaler_beats.transform(x[beat_key][:, None])), axis=1).to_numpy()\n",
    "df_demo_data.loc[df_demo_data.index,out_key+'_scaled'] = df_demo_data.apply(lambda x: np.concatenate(scaler_out.transform(np.array([x[out_key]])[:, None]))[0], axis=1).to_numpy()\n",
    "for tmp_key, tmp_count in zip(feat_keys, range(len(feat_keys))):\n",
    "    df_demo_data.loc[df_demo_data.index, tmp_key+'_scaled'] = df_demo_data.apply(lambda x: np.concatenate(scaler_X[tmp_count].transform(np.array([x[tmp_key]])[:, None])), axis=1).to_numpy()\n",
    "\n",
    "# Fetch scaled feature names\n",
    "X_keys = [a+'_scaled' for a in feat_keys]\n",
    "\n",
    "# Prepare train/test using minimal training the BP\n",
    "# Fetch data shapes\n",
    "length_seq_x = df_demo_data.apply(lambda x: len(x[beat_key+'_scaled']), axis=1).unique()[0]\n",
    "# Set the length of the target to 1\n",
    "length_seq_y = 1\n",
    "\n",
    "# Start with all points\n",
    "# Reshape the scaled beat data into a 2D array where each row corresponds to a sample and each column corresponds to a time point in the beat sequence\n",
    "# The same is done for the features and the target\n",
    "all_beats = np.reshape(np.concatenate(df_demo_data[beat_key+'_scaled'].values), (len(df_demo_data), length_seq_x))\n",
    "[all_feat1, all_feat2, all_feat3] = [df_demo_data[a].values[:, None] for a in X_keys]\n",
    "all_out = df_demo_data[out_key+'_scaled'].values[:, None]\n",
    "\n",
    "# Used only for plotting purposes\n",
    "out_max_rescaled = np.concatenate(scaler_out.inverse_transform(all_out[:, 0][:, None])).max()\n",
    "out_min_rescaled = np.concatenate(scaler_out.inverse_transform(all_out[:, 0][:, None])).min()\n",
    "\n",
    "# Given different trials have time gaps, ignore first 3 instances from indices to prevent discontiunity in training\n",
    "list_all_length = [0]\n",
    "for _, df_tmp in df_demo_data.groupby(['trial_id']):\n",
    "    list_all_length.append(len(df_tmp))\n",
    "ix_ignore_all = np.concatenate(np.array([np.arange(a, a+3,1) for a in list(np.cumsum(list_all_length)[:-1])]))\n",
    "\n",
    "# Update the final indices set\n",
    "ix_all=list(set(np.arange(len(df_demo_data)))-set(ix_ignore_all))\n",
    "\n",
    "# Separate train/test based on minimal training criterion\n",
    "random.seed(0)\n",
    "bp_dist = df_demo_data[out_key].values\n",
    "\n",
    "# Find indices for train and test datasets\n",
    "# The target values are sorted in ascending order, and the sorted indices are split into multiple subsets\n",
    "# For each subset, a random index is selected as a training index\n",
    "ix_split = np.split([a for a in np.argsort(bp_dist) if a not in set(ix_ignore_all)], np.cumsum(np.histogram(bp_dist[ix_all],bins=np.arange(bp_dist[ix_all].min(), bp_dist[ix_all].max(), 1))[0]))\n",
    "ix_train = [random.Random(4).choice(a) if len(a)>0 else -1 for a in ix_split]\n",
    "ix_train = list(set(ix_train)-set([-1]))\n",
    "\n",
    "# Test set is all remaining points not used for training\n",
    "ix_test = list(set(ix_all) - set(ix_train))\n",
    "\n",
    "# Build train and test datasets based on the indices\n",
    "train_beats = all_beats[ix_train, :]\n",
    "test_beats = all_beats[ix_test, :]\n",
    "[train_feat1, train_feat2, train_feat3] = [all_feat1[ix_train, :], all_feat2[ix_train, :], all_feat3[ix_train, :]]\n",
    "[test_feat1, test_feat2, test_feat3] = [all_feat1[ix_test, :], all_feat2[ix_test, :], all_feat3[ix_test, :]]\n",
    "train_out = all_out[ix_train, :]\n",
    "test_out = all_out[ix_test, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab85496f-ad0e-4413-8ad5-9c84f38ba47e",
   "metadata": {
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1734438633681,
     "user": {
      "displayName": "Hyounggyu Kim",
      "userId": "06810140716652608120"
     },
     "user_tz": -540
    },
    "id": "ab85496f-ad0e-4413-8ad5-9c84f38ba47e"
   },
   "outputs": [],
   "source": [
    "#### Define model input tensors\n",
    "# The training, testing, and all data are converted to TensorFlow tensors\n",
    "# The tensors for the different datasets are grouped into lists\n",
    "\n",
    "model_inp = tf.convert_to_tensor(train_beats, dtype=tf.float32)\n",
    "feat1_inp = tf.convert_to_tensor(train_feat1, dtype=tf.float32)\n",
    "feat2_inp = tf.convert_to_tensor(train_feat2, dtype=tf.float32)\n",
    "feat3_inp = tf.convert_to_tensor(train_feat3, dtype=tf.float32)\n",
    "inp_comb = [model_inp, feat1_inp, feat2_inp, feat3_inp]\n",
    "\n",
    "model_inp_test = tf.convert_to_tensor(test_beats, dtype=tf.float32)\n",
    "feat1_inp_test = tf.convert_to_tensor(test_feat1, dtype=tf.float32)\n",
    "feat2_inp_test = tf.convert_to_tensor(test_feat2, dtype=tf.float32)\n",
    "feat3_inp_test = tf.convert_to_tensor(test_feat3, dtype=tf.float32)\n",
    "inp_comb_test = [model_inp_test, feat1_inp_test, feat2_inp_test, feat3_inp_test]\n",
    "\n",
    "model_inp_all = tf.convert_to_tensor(all_beats, dtype=tf.float32)\n",
    "feat1_inp_all = tf.convert_to_tensor(all_feat1, dtype=tf.float32)\n",
    "feat2_inp_all = tf.convert_to_tensor(all_feat2, dtype=tf.float32)\n",
    "feat3_inp_all = tf.convert_to_tensor(all_feat3, dtype=tf.float32)\n",
    "inp_comb_all = [model_inp_all, feat1_inp_all, feat2_inp_all, feat3_inp_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e868349-c5ed-4c6c-9b2a-666e84cd1439",
   "metadata": {
    "id": "1e868349-c5ed-4c6c-9b2a-666e84cd1439"
   },
   "source": [
    "### Train the Conventional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543d519-1fd8-47c3-bee8-2fd9d2afe632",
   "metadata": {
    "id": "7543d519-1fd8-47c3-bee8-2fd9d2afe632"
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "###### Conventional model\n",
    "#############################\n",
    "\n",
    "# A Deep Neural Network model is initialized with the dimension of the beats, the diemnsion of each feature, and the number of neurons in the first dense layer\n",
    "model_dnn_conv = model_DNN(np.shape(train_beats)[-1], 1, 64)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "# Two lists are initialized to keep track of the training and testing loss during each epoch\n",
    "loss_list_conv = []\n",
    "test_loss_list_conv = []\n",
    "\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        tape.watch(inp_comb)\n",
    "        # Traditional out\n",
    "        yh = model_dnn_conv(inp_comb, training=True)\n",
    "        loss_ini = yh - train_out\n",
    "        loss = tf.reduce_mean(tf.square(loss_ini))\n",
    "\n",
    "    grads = tape.gradient(loss, model_dnn_conv.trainable_weights)\n",
    "\n",
    "    loss_list_conv.append(float(loss))\n",
    "    loss_final = np.min(loss_list_conv)\n",
    "    optimizer.apply_gradients(zip(grads, model_dnn_conv.trainable_weights))\n",
    "\n",
    "    pred_out = model_dnn_conv(inp_comb_test)\n",
    "\n",
    "    test_loss_ini = pred_out - test_out\n",
    "    test_loss = tf.reduce_mean(tf.square(test_loss_ini))\n",
    "    test_loss_list_conv.append(float(test_loss))\n",
    "\n",
    "    # If the training loss reaches a minimum value of 0.01, or the maximum number of epochs is reached, the training process is stopped\n",
    "    if (loss_final<=0.01) | (epoch==epochs-1):\n",
    "        print(\"Conventional model training Completed. Epoch %d/%d -- loss: %.4f\" % (epoch, epochs, float(loss)))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c83887-d459-4504-9904-146bac14cfb3",
   "metadata": {
    "id": "72c83887-d459-4504-9904-146bac14cfb3"
   },
   "source": [
    "### Train the PINN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ed78b-d46e-4fa8-b1a0-8799d4f5eed9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "executionInfo": {
     "elapsed": 94714,
     "status": "error",
     "timestamp": 1734438247574,
     "user": {
      "displayName": "Hyounggyu Kim",
      "userId": "06810140716652608120"
     },
     "user_tz": -540
    },
    "id": "f99ed78b-d46e-4fa8-b1a0-8799d4f5eed9",
    "outputId": "15590fff-ac18-40ec-ce91-d9648d28865b"
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "############### PINN MODEL\n",
    "#############################\n",
    "\n",
    "# A Deep Neural Network model is initialized with the dimension of the beats, the diemnsion of each feature, and the number of neurons in the first dense layer\n",
    "\n",
    "model_dnn_pinn = model_DNN(np.shape(train_beats)[-1], 1, 64)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10e-4)\n",
    "\n",
    "# Two lists are initialized to keep track of the training and testing loss during each epoch\n",
    "loss_list_pinn = []\n",
    "test_loss_list_pinn = []\n",
    "\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        tape.watch(inp_comb)\n",
    "        # Traditional out\n",
    "        yh = model_dnn_pinn(inp_comb, training=True)\n",
    "        loss_ini = yh - train_out\n",
    "        loss = tf.reduce_mean(tf.square(loss_ini))\n",
    "\n",
    "        # Additional tf.GradientTape contexts are used to compute the derivatives of the model's predictions with respect to the features\n",
    "        with tf.GradientTape() as deriv_f1:\n",
    "            deriv_f1.watch(inp_comb_all)\n",
    "            yhp = model_dnn_pinn(inp_comb_all, training=True)\n",
    "        dx_f1 = deriv_f1.gradient(yhp, feat1_inp_all)\n",
    "\n",
    "        with tf.GradientTape() as deriv_f2:\n",
    "            deriv_f2.watch(inp_comb_all)\n",
    "            yhp = model_dnn_pinn(inp_comb_all, training=True)\n",
    "        dx_f2 = deriv_f2.gradient(yhp, feat2_inp_all)\n",
    "\n",
    "        with tf.GradientTape() as deriv_f3:\n",
    "            deriv_f3.watch(inp_comb_all)\n",
    "            yhp = model_dnn_pinn(inp_comb_all, training=True)\n",
    "        dx_f3 = deriv_f3.gradient(yhp, feat3_inp_all)\n",
    "\n",
    "        # A physics-based prediction is computed by adding the model's predictions to the product of the computed derivatives and\n",
    "        # the differences in the feature values between consecutive timesteps\n",
    "        pred_physics = (yhp[:-1, 0]\n",
    "                        +tf.keras.layers.Multiply()([dx_f1[:-1, 0], feat1_inp_all[1:, 0] - feat1_inp_all[:-1, 0]])\n",
    "                        +tf.keras.layers.Multiply()([dx_f2[:-1, 0], feat2_inp_all[1:, 0] - feat2_inp_all[:-1, 0]])\n",
    "                        +tf.keras.layers.Multiply()([dx_f3[:-1, 0], feat3_inp_all[1:, 0] - feat3_inp_all[:-1, 0]])\n",
    "                        )\n",
    "\n",
    "        physics_loss_ini = pred_physics - yhp[1:, 0]\n",
    "        physics_loss = tf.reduce_mean(tf.square(tf.gather_nd(physics_loss_ini,indices = np.array(ix_all[:-1])[:, None])))\n",
    "\n",
    "        # The total loss is computed as the sum of the initial loss and ten times the physics-based loss\n",
    "        # The physics-based loss is multiplied by a factor of ten to emphasize its importance in the loss function\n",
    "        loss_total = loss + physics_loss * 10\n",
    "\n",
    "    grads = tape.gradient(loss_total, model_dnn_pinn.trainable_weights)\n",
    "\n",
    "    loss_list_pinn.append(float(loss))\n",
    "    loss_final=np.min(loss_list_pinn)\n",
    "    optimizer.apply_gradients(zip(grads, model_dnn_pinn.trainable_weights))\n",
    "\n",
    "    pred_out = model_dnn_pinn(inp_comb_test)\n",
    "    test_loss_ini = pred_out - test_out\n",
    "    test_loss = tf.reduce_mean(tf.square(test_loss_ini))\n",
    "    test_loss_list_pinn.append(float(test_loss))\n",
    "\n",
    "    print(\"PINN model training Completed. Epoch %d/%d -- loss: %.4f\" % (epoch,epochs,float(loss)))\n",
    "\n",
    "    # If the training loss reaches a minimum value of 0.01, or the maximum number of epochs is reached, the training process is stopped\n",
    "    if (loss_final<=0.03) | (epoch==epochs-1):\n",
    "        print(\"PINN model training Completed. Epoch %d/%d -- loss: %.4f\" % (epoch,epochs,float(loss)))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776218f-592d-4278-9ed6-da9326f2fd66",
   "metadata": {
    "id": "7776218f-592d-4278-9ed6-da9326f2fd66"
   },
   "outputs": [],
   "source": [
    "#The trained model's predictions on the test dataset are computed\n",
    "pred_out = model_dnn_conv(inp_comb_test)\n",
    "\n",
    "#The Pearson correlation coefficient and the Root Mean Square Error are calculated between the actual and predicted test outcomes\n",
    "corr_conv = np.corrcoef(np.concatenate(test_out)[:], np.concatenate(pred_out)[:])[0][1]\n",
    "rmse_conv = np.sqrt(np.mean(np.square\n",
    "                           (np.concatenate(scaler_out.inverse_transform(np.concatenate(test_out)[:][:, None]))-\n",
    "                            np.concatenate(scaler_out.inverse_transform(np.concatenate(pred_out)[:][:, None])))))\n",
    "\n",
    "pred_out = model_dnn_pinn(inp_comb_test)\n",
    "corr_pinn = np.corrcoef(np.concatenate(test_out)[:], np.concatenate(pred_out)[:])[0][1]\n",
    "rmse_pinn = np.sqrt(np.mean(np.square(\n",
    "    np.concatenate(scaler_out.inverse_transform(np.concatenate(test_out)[:][:, None]))-\n",
    "    np.concatenate(scaler_out.inverse_transform(np.concatenate(pred_out)[:][:, None])))))\n",
    "\n",
    "print('#### Conventional Performance ####')\n",
    "print('Corr: %.2f,  RMSE: %.1f'%(corr_conv, rmse_conv))\n",
    "print('----------------------------------')\n",
    "print('#### PINN Performance ####')\n",
    "print('Corr: %.2f,  RMSE: %.1f'%(corr_pinn, rmse_pinn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77d2b1-8f28-4d2c-a5e9-367dac4dc509",
   "metadata": {
    "id": "bf77d2b1-8f28-4d2c-a5e9-367dac4dc509"
   },
   "outputs": [],
   "source": [
    "s=figure(width=770,height=400,y_range=(out_min_rescaled-20,out_max_rescaled+20))\n",
    "s.scatter(ix_test,np.concatenate(scaler_out.inverse_transform(np.concatenate(model_dnn_conv(inp_comb_test))[:,None])),size=7,line_color=None,color=palettes.Colorblind8[5],legend_label='Conv.')\n",
    "s.scatter(ix_test,np.concatenate(scaler_out.inverse_transform(np.concatenate(model_dnn_pinn(inp_comb_test))[:,None])),size=7,line_color=None,color=palettes.Colorblind8[3],legend_label='PINN')\n",
    "s.line(list(range(len(df_demo_data))),np.concatenate(scaler_out.inverse_transform(all_out[:,0][:,None])),line_width=3,line_color='black',line_alpha=1,line_dash='dashed',legend_label='True BP')\n",
    "s.xaxis.axis_label='Beat time (s)'\n",
    "s.yaxis.axis_label='SBP (mmHg)'\n",
    "\n",
    "s2 = figure(width=500,height=400,y_axis_type=\"log\",y_range=(1e-2,2))\n",
    "s2.line(np.linspace(0,100,len(loss_list_conv)),loss_list_conv,line_width=3,color='black',legend_label='PINN-test')\n",
    "s2.line(np.linspace(0,100,len(test_loss_list_conv)),test_loss_list_conv,line_width=3,line_dash='dashed',color='black',legend_label='Conv-test')\n",
    "s2.line(np.linspace(0,100,len(loss_list_pinn)),loss_list_pinn,line_width=3,alpha=0.8,color='orange',legend_label='PINN-train')\n",
    "s2.line(np.linspace(0,100,len(test_loss_list_pinn)),test_loss_list_pinn,line_width=3,line_dash='dashed',color='orange',legend_label='Conv-train')\n",
    "s2.xaxis.axis_label = 'Training percent (%)'\n",
    "s2.yaxis.axis_label = 'mse norm.'\n",
    "\n",
    "figure_settings(s)\n",
    "figure_settings(s2)\n",
    "\n",
    "show(row(s,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08941756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ppg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
